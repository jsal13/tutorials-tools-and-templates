{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before doing anything, please run the following cell which will make a CSV file called \"data.csv\" in this directory and which creates a local spark session.\n",
    "\n",
    "## TODO: \n",
    "1. Put some NaNs into the DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run me!\n",
    "\n",
    "# Creates a local spark session.\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"PySpark Tutorial\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Getting and Querying the Data\n",
    "\n",
    "Here we're going to make a Spark Context.  If you need to restart it, go into \"Kernel > Restart Kernel.\"  In general, if anything bad happens spark-related, you're going to want to do that reset kernel thing.\n",
    "\n",
    "Problems will be given by section as below.  Note that you *will have to import some new modules from pyspark; not all required imports are given above*.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.1 Importing with inferSchema\n",
    "\n",
    "Import the `data.csv` file.  Use `inferSchema=True` to infer the schema.  Be sure that you can print the head of the rdd as well as printing out the schema using `.printSchema()`.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Importing with Structs\n",
    "\n",
    "Import the `data.csv` file (again).  Explicitly use StructFields to create the schema.  When would you want to explicitly define the schema types?\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 Using Select\n",
    "\n",
    "Select only `datetime_col, float_col, categorical_col`. Print 25 records without Python truncating the dataset.  Hint: for the last part, you will use a parameter in the .show() method.\n",
    "\n",
    "### 1.4 Some Example Queries of the Data\n",
    "\n",
    "1. Select only those values whose `categorical_col` is `Low` and the `int_col` value is negative.\n",
    "2. Group by categoricals, giving the sum of the `int_col` and the average of the `int_col` as new columns.\n",
    "3. Group by categoricals, giving the sum of the `int_col` and the average of the `int_col` as new columns, and show only the ones having an average greater than 0.\n",
    "4. Count the number of times each category comes up in the `categorical_col` column.  The result should be two columns: the category and the count.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Manipulating RDDs\n",
    "\n",
    "This section will describe how to using maps, casting, etc., to transform your data or creat new columns.\n",
    "\n",
    "### 2.1 Casting\n",
    "\n",
    "1. Cast `int_col` as DoubleType and check the dtypes of the RDD. (Cast them back when you're done!)\n",
    "\n",
    "### 2.2 User Defined Functions and Mappings.\n",
    "\n",
    "1. Create a small list of first names.  Use these to create a Spark Data Frame which has a single column.  (Hint: this is sort of a strange one with a single column, if you can't get it you can check the solutions for one way to do it.)\n",
    "\n",
    "2. Create a small list of first names and last names.  Use these to create a Spark Data Frame which has two columns (first, last). \n",
    "\n",
    "3. Let's use the Spark DataFrame from the previous question (2.2.2).  Create a new column which is the concatenation of the first letters of the first and last name.  For example, \"Kara\", \"Williams\" would give the value \"KW\".  Use a UDF to help you here.\n",
    "\n",
    "4. Using the Spark DataFrame from the previous question (2.2.3), modify the \"initials\" column to be lowercase.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: ML and Pipelines.\n",
    "\n",
    "These exercises will guide you through building a simple ML Pipeline with the a toy dataset.\n",
    "\n",
    "1. Load `./ml_features.csv` and `./ml_labels.csv` into two Spark DataFrames, `features` and `labels`, respectively.\n",
    "\n",
    "2. \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Make a ml_features, ml_labels thing to do logistic regression.\n",
    "# Needs to do a pipeline stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete this stuff after this; used for debugging.\n",
    "\n",
    "file_loc = \"./data.csv\"\n",
    "sdf = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", True) \\\n",
    "        .option(\"inferSchema\", True) \\\n",
    "        .load(file_loc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
